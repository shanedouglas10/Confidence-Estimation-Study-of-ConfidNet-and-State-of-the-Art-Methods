import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision.datasets import CIFAR10, CIFAR100, STL10
from torchvision import transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

class ConfidNet(nn.Module):
    def __init__(self, num_classes=10, debug=False):
        super(ConfidNet, self).__init__()
        self.debug = debug
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        feature_size = 64 * 16 * 16
        self.fc1 = nn.Linear(feature_size, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.confidence_head = nn.Linear(128, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        if self.debug:
            self.debug = False
        x = torch.flatten(x, 1)
        features = F.relu(self.fc1(x))
        classification_outputs = self.fc2(features)
        confidence_outputs = torch.sigmoid(self.confidence_head(features))
        return classification_outputs, confidence_outputs

def combined_loss(classification_outputs, confidence_outputs, labels):
    classification_loss = F.cross_entropy(classification_outputs, labels)
    correctness = (classification_outputs.argmax(dim=1) == labels).float()
    confidence_loss = F.mse_loss(confidence_outputs.squeeze(), correctness)
    return classification_loss + confidence_loss

transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset_cifar10 = CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset_cifar10 = CIFAR10(root='./data', train=False, download=True, transform=transform)

train_dataset_cifar100 = CIFAR100(root='./data', train=True, download=True, transform=transform)
test_dataset_cifar100 = CIFAR100(root='./data', train=False, download=True, transform=transform)

train_dataset_stl10 = STL10(root='./data', split='train', download=True, transform=transform)
test_dataset_stl10 = STL10(root='./data', split='test', download=True, transform=transform)

batch_size = 64
data_loaders = {
    "CIFAR-10": {
        "train": DataLoader(train_dataset_cifar10, batch_size=batch_size, shuffle=True),
        "test": DataLoader(test_dataset_cifar10, batch_size=batch_size, shuffle=False)
    },
    "CIFAR-100": {
        "train": DataLoader(train_dataset_cifar100, batch_size=batch_size, shuffle=True),
        "test": DataLoader(test_dataset_cifar100, batch_size=batch_size, shuffle=False)
    },
    "STL-10": {
        "train": DataLoader(train_dataset_stl10, batch_size=batch_size, shuffle=True),
        "test": DataLoader(test_dataset_stl10, batch_size=batch_size, shuffle=False)
    }
}

def train_confidnet(dataset_name, train_loader, test_loader, device):
    num_classes = 10 if "CIFAR-10" in dataset_name or "STL-10" in dataset_name else 100
    model = ConfidNet(num_classes=num_classes, debug=True).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    epochs = 5
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            classification_outputs, confidence_outputs = model(images)
            loss = combined_loss(classification_outputs, confidence_outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}")
    evaluate_model(model, test_loader, dataset_name, device)
    return model

def evaluate_model(model, test_loader, dataset_name, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            classification_outputs, confidence_outputs = model(images)
            _, predicted = torch.max(classification_outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    accuracy = 100 * correct / total
    print(f"Test Accuracy on {dataset_name}: {accuracy:.2f}%")

def analyze_confidence(test_loader, model, device):
    correct_confidences = []
    incorrect_confidences = []
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            classification_outputs, confidence_outputs = model(images)
            _, predicted = torch.max(classification_outputs, 1)
            correctness = (predicted == labels).cpu().numpy()
            confidences = confidence_outputs.squeeze().cpu().numpy()
            for i in range(len(correctness)):
                if correctness[i]:
                    correct_confidences.append(confidences[i])
                else:
                    incorrect_confidences.append(confidences[i])
    return correct_confidences, incorrect_confidences

def visualize_confidence(dataset_name, test_loader, model, device):
    correct_confidences, incorrect_confidences = analyze_confidence(test_loader, model, device)
    plt.hist(correct_confidences, bins=20, alpha=0.7, label='Correct Predictions')
    plt.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect Predictions')
    plt.xlabel('Confidence')
    plt.ylabel('Frequency')
    plt.legend()
    plt.title(f'Confidence Distribution for {dataset_name}')
    plt.show()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for dataset_name, loaders in data_loaders.items():
    train_loader = loaders["train"]
    test_loader = loaders["test"]
    model = train_confidnet(dataset_name, train_loader, test_loader, device)
    visualize_confidence(dataset_name, test_loader, model, device)
