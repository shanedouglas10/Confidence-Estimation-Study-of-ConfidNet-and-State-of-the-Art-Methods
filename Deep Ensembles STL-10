import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.num_classes = num_classes
        self.fc1 = None
        self.fc2 = None

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(torch.relu(self.conv2(x)))
        if self.fc1 is None:
            flattened_size = x.size(1) * x.size(2) * x.size(3)
            self.fc1 = nn.Linear(flattened_size, 128).to(x.device)
            self.fc2 = nn.Linear(128, self.num_classes).to(x.device)
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train_deep_ensemble(num_models, train_loader, num_classes, device):
    models = []
    for model_idx in range(1, num_models + 1):
        print(f"Training model {model_idx}/{num_models}...")
        model = SimpleCNN(num_classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        epochs = 5
        for epoch in range(epochs):
            model.train()
            running_loss = 0.0
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            print(f"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}")
        models.append(model)
    return models

def evaluate_ensemble(models, test_loader, num_classes, device):
    correct_confidences = []
    incorrect_confidences = []
    total_correct = 0
    total_samples = 0
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        ensemble_outputs = torch.zeros(images.size(0), num_classes).to(device)
        for model in models:
            model.eval()
            with torch.no_grad():
                ensemble_outputs += model(images)
        ensemble_outputs /= len(models)
        confidences, predictions = torch.max(torch.softmax(ensemble_outputs, dim=1), dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)
        for i in range(len(labels)):
            if predictions[i] == labels[i]:
                correct_confidences.append(confidences[i].item())
            else:
                incorrect_confidences.append(confidences[i].item())
    accuracy = 100 * total_correct / total_samples
    print(f"Ensemble Accuracy on STL-10: {accuracy:.2f}%")
    return correct_confidences, incorrect_confidences

def plot_confidence_distribution(correct_confidences, incorrect_confidences):
    plt.hist(correct_confidences, bins=20, alpha=0.7, label="Correct Predictions")
    plt.hist(incorrect_confidences, bins=20, alpha=0.7, label="Incorrect Predictions")
    plt.xlabel("Confidence")
    plt.ylabel("Frequency")
    plt.title("Confidence Distribution for Deep Ensembles")
    plt.legend()
    plt.show()

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_classes = 10
    num_models = 5
    transform = transforms.Compose([
        transforms.Resize((96, 96)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    train_dataset = datasets.STL10(root='./data', split='train', download=True, transform=transform)
    test_dataset = datasets.STL10(root='./data', split='test', download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    ensemble_models = train_deep_ensemble(num_models, train_loader, num_classes, device)
    correct_confidences, incorrect_confidences = evaluate_ensemble(ensemble_models, test_loader, num_classes, device)
    plot_confidence_distribution(correct_confidences, incorrect_confidences)
