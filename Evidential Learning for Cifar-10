import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load CIFAR-10
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)

num_classes = 10

# Simple CNN Model for EDL
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 16 * 16, 256)
        self.fc2 = nn.Linear(256, num_classes)
        self.softplus = nn.Softplus()

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 16 * 16)
        x = F.relu(self.fc1(x))
        evidence = self.softplus(self.fc2(x))
        return evidence

# EDL Utility Functions
def one_hot(labels, num_classes):
    return F.one_hot(labels, num_classes=num_classes).float()

def dirichlet_params(evidence):
    alpha = evidence + 1.0
    alpha = torch.clamp(alpha, min=1e-3)
    alpha0 = torch.sum(alpha, dim=1, keepdim=True)
    return alpha, alpha0

def edl_loss(y, alpha, alpha0, lambda_reg=1e-5):
    mean_prob = alpha / alpha0
    mse = torch.mean(torch.sum((y - mean_prob)**2, dim=1))
    K = alpha.shape[1]
    beta = torch.ones((1, K), device=alpha.device)
    alpha_clamped = torch.clamp(alpha, min=1e-3)
    alpha0_sum = torch.sum(alpha_clamped - 1, dim=1, keepdim=True)
    beta0_sum = torch.sum(beta - 1, dim=1, keepdim=True)
    kl = torch.mean(
        (torch.lgamma(alpha0_sum + 1e-5) - torch.sum(torch.lgamma(alpha_clamped + 1e-5), dim=1, keepdim=True))
        - (torch.lgamma(beta0_sum + 1e-5) - torch.sum(torch.lgamma(beta + 1e-5), dim=1, keepdim=True))
        + torch.sum((alpha_clamped - beta) *
                    (torch.digamma(alpha_clamped + 1e-5) - torch.digamma(alpha0_sum + 1e-5)), dim=1, keepdim=True)
    )
    return mse + lambda_reg * kl

def train_evidential(model, train_loader, device, epochs=5, lr=1e-4):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.train()
    for epoch in range(1, epochs + 1):
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            evidence = model(inputs)
            alpha, alpha0 = dirichlet_params(evidence)
            y = one_hot(targets, num_classes)
            loss = edl_loss(y, alpha, alpha0, lambda_reg=1e-5)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            running_loss += loss.item() * targets.size(0)
        print(f"Epoch [{epoch}/{epochs}] Loss: {running_loss / len(train_loader.dataset):.4f}")

def evaluate_evidential(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    all_probs = []
    all_targets = []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            evidence = model(inputs)
            alpha, alpha0 = dirichlet_params(evidence)
            mean_prob = alpha / alpha0
            preds = torch.argmax(mean_prob, dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)
            all_probs.append(mean_prob.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    acc = correct / total
    all_probs = np.concatenate(all_probs, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    return acc, all_targets, all_probs

# Train 5 EDL Models
models = []
for i in range(5):
    print(f"\nTraining EDL Model {i + 1}/5")
    model = SimpleCNN(num_classes=num_classes).to(device)
    train_evidential(model, train_loader, device, epochs=5, lr=1e-4)
    models.append(model)

# Ensemble Predictions
print("\nEvaluating Ensemble of EDL Models...")
all_targets = []
all_ensemble_probs = []
with torch.no_grad():
    for inputs, targets in test_loader:
        inputs = inputs.to(device)
        targets_np = targets.numpy()
        ensemble_outputs = []
        for model in models:
            evidence = model(inputs)
            alpha, alpha0 = dirichlet_params(evidence)
            mean_prob = (alpha / alpha0).cpu().numpy()
            ensemble_outputs.append(mean_prob)
        ensemble_outputs = np.mean(ensemble_outputs, axis=0)
        all_ensemble_probs.append(ensemble_outputs)
        all_targets.append(targets_np)

all_ensemble_probs = np.concatenate(all_ensemble_probs, axis=0)
all_targets = np.concatenate(all_targets, axis=0)
preds = np.argmax(all_ensemble_probs, axis=1)
ensemble_acc = np.mean(preds == all_targets)
print(f"Ensemble Test Accuracy: {ensemble_acc * 100:.2f}%")

correct_indices = np.where(preds == all_targets)[0]
incorrect_indices = np.where(preds != all_targets)[0]
correct_confidences = np.max(all_ensemble_probs[correct_indices], axis=1)
incorrect_confidences = np.max(all_ensemble_probs[incorrect_indices], axis=1)

plt.hist(correct_confidences, bins=20, alpha=0.7, label='Correct Predictions')
plt.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect Predictions')
plt.xlabel("Confidence")
plt.ylabel("Frequency")
plt.title("Confidence Distribution (EDL Ensemble on CIFAR-10)")
plt.legend()
plt.show()
